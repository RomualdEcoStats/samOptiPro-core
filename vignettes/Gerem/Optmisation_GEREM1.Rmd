---
title: "GEREM"
author: "ICES Data Group"
date: "08/09/2020"
documentclass: article
output: 
  bookdown::word_document2:
    fig_caption: yes
    number_sections: no
    reference_docx: "../../Rmarkdown/ICES_template.docx"
bibliography: gerem.bib
csl: "../../Rmarkdown/ices-journal-of-marine-science.csl"
---

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = FALSE,warning=FALSE,message=FALSE,fig.width=14.9/2.54,
                      fig.height=10/2.54,dpi=300)
colorpalette=cbf_1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73", 
                        "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

load("glass_eel_yoy.Rdata")
series=unique(glass_eel_yoy$site)

username = Sys.info()["user"]
CY<-2024 # current year ==> dont forget to update the graphics path below (

library("RPostgres")
library("hues")
library("bookdown")
library("coda")
library("nimble")
library("getPass")
library("RPostgres")
library("sf")
library("dplyr")
library("ggplot2")
library("yaml")
library("parallel")
library("tidyr")
library("flextable")
library("reshape2")
library("rnaturalearth")


set_flextable_defaults(
  font.size = 8.5, font.family = "Calibri",
  font.color = "black",
  table.layout = "autofit",
  border.color = "black",
  theme_fun=theme_booktabs)


knitr::opts_chunk$set(echo = TRUE)
updatedData = FALSE
if ((!file.exists(paste("datagerem_nimble2_",CY,".rdata",sep="")))){
  if (username=="hdrouineau"){
    cred_ccm=read_yaml("credentials_ccm.yml")
    con_ccm=dbConnect(Postgres(),host=cred_ccm$host,dbname=cred_ccm$dbname,
                      user=cred_ccm$user,port=cred_ccm$port,
                      password=cred_ccm$password)
    table_seaoutlets="hydrographie.ccm_v2_1_riverbasin_seaoutlets"
  }
  
  #get the surface of a catchment given a list of wso_id
  getSurface=function(w,con){
    if (is.na(w)) return(2.133000e+16) #corresponds to unreal catchments
    w <- gsub("\\{","(",w)
    w <- gsub("\\}",")",w)
    dbGetQuery(con,paste("select sum(area_km2) from",
                         table_seaoutlets,
                         "where wso_id in ",w))[1,1]
  }
  cred=read_yaml("credentials.yml")
  
  
  con_wgeel=dbConnect(Postgres(),
                      dbname=cred$dbname,
                      host=cred$host,
                      port=cred$port,
                      user= cred$user,
                      password= cred$password)
  updatedData = TRUE
  
} else {
  load(paste("datagerem_nimble2_",CY,".rdata",sep=""))
}

create_zone=function(z, izone,tab_series_zones){
  # tab_series_zones <- tab_series %>%
  #   dplyr::filter(zone == z)
  
  #list of all ccm_wso_id: some are too small and are not in all catchments
  catchment_with_series=as.integer(unique(na.omit(unlist(stringr::str_split(gsub("\\{","",gsub("\\}","",tab_series_zones$ser_ccm_wso_id)),",")))))
  allcatchment_zone=allcatchments%>%st_drop_geometry()%>% filter(zone==z)
  catchments_zone_series=allcatchment_zone %>%
    filter(ser_ccm_wso_id%in%catchment_with_series)
  catchments_zone_not_in_series=allcatchment_zone %>%
    filter(!ser_ccm_wso_id%in%catchment_with_series)
  if (length(catchment_with_series) > 1) {
    weights<-paste0("weights_z",izone,"[1:(1+length(surface_catchments_zone_series_z",izone,"))] <- c(pow(surface_catchments_zone_series_z",izone,"[1:(length(surface_catchments_zone_series_z",izone,"))],beta), sum(pow(surface_catchments_not_in_series_z",izone,"[1:(length(surface_catchments_not_in_series_z",izone,"))],beta)))")
    alphaZ<-paste0("alpha_z",izone,"[1:(1+length(surface_catchments_zone_series_z",izone,"))] <- weights_z",izone,"[1:(1+length(surface_catchments_zone_series_z",izone,"))] / sum(weights_z",izone,"[1:(1+length(surface_catchments_zone_series_z",izone,"))]) * concentration_zone")
    props<-paste0("for (y in 1:nbyear){
  props_z",izone,"[y,1:(1+length(surface_catchments_zone_series_z",izone,"))] ~ ddirich(alpha_z",izone,"[1:(1+length(surface_catchments_zone_series_z",izone,"))])
}")
    
  } else {
    weights <- paste0("weights_z",izone, "[1:2] <- c(pow(surface_catchments_zone_series_z",izone,",beta), sum(pow(surface_catchments_not_in_series_z",izone,"[1:(length(surface_catchments_not_in_series_z",izone,"))],beta)))")
    alphaZ<-paste0("alpha_z",izone,"[1:2] <- weights_z",izone,"[1:2] / sum(weights_z",izone,"[1:2]) * concentration_zone")
    props<-paste0("for (y in 1:nbyear){
  props_z",izone,"[y,1:2] ~ ddirich(alpha_z",izone,"[1:2])
}")
  }
  
  
  
  
  Rcm <- 'for (y in 1:nbyear){'
  for (i in 1:nrow(tab_series_zones)){
    ser_id=match(as.integer(unique(na.omit(unlist(stringr::str_split(gsub("\\{","",gsub("\\}","",tab_series_zones$ser_ccm_wso_id[i])),","))))),catchments_zone_series$ser_ccm_wso_id)
    if (length(ser_id)>0){
      if (all(!is.na(ser_id))){
        Rcm <-paste(Rcm,paste0('    Rcm[y,',match(tab_series_zones$ser_ccm_wso_id[i],tab_series$ser_ccm_wso_id),"] <- ",
                               paste(paste0("props_z",izone,"[y,",ser_id,"]*Rzone[y,",izone,"]"), collapse = "+")),sep="\n")
      } else{
        Rcm <-paste(Rcm,paste0('  Rcm[y,',match(tab_series_zones$ser_ccm_wso_id[i],tab_series$ser_ccm_wso_id),"] <- Rzone[y,",izone,"]"),sep="\n")
      }
    } else{
      Rcm <-paste(Rcm,paste0('  Rcm[y,',match(tab_series_zones$ser_ccm_wso_id[i],tab_series$ser_ccm_wso_id),"] <- Rzone[y,",izone,"]"),sep="\n")
    }
  }
  Rcm <- paste(Rcm,'}', sep="\n")
  res <- list(code=paste(paste("#####Zone",izone),
                         weights,
                         alphaZ,
                         props,
                         Rcm,
                         sep="\n\n"),
              surface_catchments_zone_series_z=catchments_zone_series$area_km2,
              surface_catchments_not_in_series_z=catchments_zone_not_in_series$area_km2)
  names(res) <- c("code",
                  paste0("surface_catchments_zone_series_z",izone),
                  paste0("surface_catchments_not_in_series_z",izone))
  res
  
}


```

# Introduction
GEREM is a Bayesian model aiming at estimating glass eel recruitment at different nested spatial scales (overall recruitment, sub-regions/zone, river basins) through the analysis of available recruitment time series [@drouineau2016]. The model has already been applied in France [@drouineau2016], to a large part of Europe [@bornarel2018a] and a specific application was carried out in the context of the Sudoang Interreg project [@drouineau2021]. It had been used by WGEEL few years ago [@ices2020a] and was updated during WGEEL last year. It was decided to renew the exercise since GEREM is a candidate to feed the spatial assessment model promoted in the WKFEA roadmap [@ices2021] and is a good example of the hierarchy of spatial scales on which would be based such as spatial model. The model assumes that each year, the overall recruitment $R(y)$ is distributed among various zones (i.e. subregions) which receive recruitment $R_z(y)$. Then, zone recruitment is distributed among river catchments as a function of their surface, leading to recruitment $R_{c,z}(y)$. Basically, GEREM is a mixing of a Dynamic Factor Analysis (DFA) [@zuur2003a] and a “rule of three”. Similarly to a DFA model, GEREM is state-space model based on a random walk structure, which estimates common trends in a set of time series. The rule of three is used to extrapolate absolute recruitment estimates in a river basin to recruitment in other basins in the same zone, stating that the recruitment in each basin is a simple function of its surface. After having inventoried available time series and listed their characteristics, it is necessary to define zones. In each zone: 

* river catchments should have similar trends in recruitment 
* the rule of three must apply, i.e. it should be possible to extrapolate recruitment in a basin to another basin of the same zone as a simple function of their relative surfaces 
* time series of recruitment should be available. Morevover, there should be at least on time series of absolute recruitment. If not available, it is possible to use time series such as trapping or commercial catch from which absolute recruitment can be inferred by introducing additional information on the scaling factors (trap efficiency and exploitation rate).

The model is detailed in [@drouineau2016] and [@bornarel2018a]. The current exercise is mainly an update from [@bornarel2018a]. We will use the same zone and the nearly the same time series but with updated values.


# Material and Methods

```{r dataLoadingFormatting, echo=FALSE, warning=FALSE, message=FALSE}
####loading time series from WGEEL
if ((!file.exists(paste("datagerem_nimble2_",CY,".rdata",sep=""))) ){
  
  series_wgeel=read.table("catchment_wgeel.csv",header=TRUE,sep=";")
  wgeel=dbGetQuery(con_wgeel,paste("select das_year,ser_nameshort,ser_uni_code,das_value,ser_ccm_wso_id,ser_x,ser_y from datawg.t_dataseries_das left join datawg.t_series_ser on das_ser_id =ser_id where das_year>=1960 and das_year <=",CY," and ser_nameshort in ('",paste(series_wgeel$ser_nameshort[series_wgeel$from_wgeel],collapse="','"),"')",sep="") )
  series_wgeel <- merge(series_wgeel,
                        unique(wgeel[,c("ser_nameshort","ser_ccm_wso_id","ser_x","ser_y")]),
                        all.x=TRUE)
  series_wgeel <- rbind.data.frame(series_wgeel,series_wgeel[series_wgeel$ser_nameshort=="MiSpG",,drop=FALSE])
  series_wgeel$ser_nameshort[nrow(series_wgeel)] = "MinG"
  series_wgeel$ser_ccm_wso_id[!series_wgeel$from_wgeel] = 
    as.character(series_wgeel$wso_id[!series_wgeel$from_wgeel])
  series_wgeel$surface=sapply(series_wgeel$ser_ccm_wso_id,getSurface,con=con_ccm )
  
  
  ###converting to kg
  wgeel$das_value=ifelse(wgeel$ser_uni_code=="t",
                         wgeel$das_value*1000,
                         ifelse(wgeel$ser_uni_code=="nr",
                                wgeel$das_value*0.3/1000,
                                wgeel$das_value))
  
  ###reshaping and merging Minho Spain and Portugal
  wgeel_wide=wgeel %>%
    dplyr::select(das_year,das_value,ser_nameshort) %>%
    pivot_wider(names_from=ser_nameshort,values_from=das_value,id_cols=das_year)
  
  
  
  
  ####loading additional french series
  french_wide=read.table("french_serie2.csv",header=TRUE,sep=";")
  names(french_wide)[1]="das_year"
  french_wide=subset(french_wide,french_wide$das_year>=1960 & french_wide$das_year<=CY)
  
  
  tmp <- unique(na.omit(series_wgeel[,c("ser_x","ser_y","ser_ccm_wso_id")]))
  series_wgeel$ser_x[!series_wgeel$from_wgeel]=
    tmp$ser_x[match(series_wgeel$ser_ccm_wso_id[!series_wgeel$from_wgeel],
                    tmp$ser_ccm_wso_id)]
  series_wgeel$ser_y[!series_wgeel$from_wgeel]=
    tmp$ser_y[match(series_wgeel$ser_ccm_wso_id[!series_wgeel$from_wgeel],
                    tmp$ser_ccm_wso_id)]
  
  series_wgeel[series_wgeel$ser_nameshort=="SeGEMAC",c("ser_x","ser_y")] =
    c(-1.136938,45.796857)
  series_wgeel[series_wgeel$ser_nameshort=="ChGEMAC",c("ser_x","ser_y")] =
    c(-1.076013,45.953153)
  series_wgeel[series_wgeel$ser_nameshort=="Somme",c("ser_x","ser_y")] =
    c(1.644597,50.181853)
  series_wgeel[series_wgeel$ser_nameshort=="Oria",c("ser_x","ser_y")] =
    c(-2.1307297,43.2827)
  
  
  values_wide=merge(wgeel_wide,french_wide,all=TRUE)
  series=series_wgeel
  
  values_long <- values_wide %>%
    pivot_longer(-das_year,
                 names_to="ser_nameshort",
                 values_to="das_value")
  
  series_stat <- merge(series, values_long) %>%
    dplyr::select(-wso_id,-ser_ccm_wso_id,-scale_bound_shape2,-scale_bound_shape1, -min_bound,-max_bound) %>%
    filter(!is.na(das_value)) %>%
    group_by(ser_nameshort, type, zone, surface) %>%
    summarise(first_year=min(das_year),
              last_year=max(das_year),
              nbyear=n_distinct(das_year))
  values_long <- na.omit(merge(values_long,
                               series[,c("ser_nameshort","zone")]))
  data_points <- values_long %>%
    group_by(zone,das_year) %>%
    summarise(n=n(),pre=n()>0)
}
```

```{r buildingZone, echo=FALSE, warning=FALSE, message=FALSE}
######building zones
if ((!file.exists(paste("datagerem_nimble2_",CY,".rdata",sep=""))) ){
  sf::sf_use_s2(FALSE)
  
  allcatchments=st_read(con_ccm,query=paste('select wso_id ser_ccm_wso_id,area_km2,"window",sea_cd,geom from',table_seaoutlets,' where strahler>0 and area_km2 > 0 and ("window"<=2004 or "window"=2008)'))
  outlets=st_read(con_ccm,query='select "window",gid, wso_id ser_ccm_wso_id, geom from hydrographie.ccm_v2_1_w2003_rivernodes where num_seg=0')
  outlets=subset(outlets,outlets$ser_ccm_wso_id %in% allcatchments$ser_ccm_wso_id)
  emu=st_read(con_wgeel,query="select emu_nameshort, emu_cou_code,geom from ref.tr_emu_emu")
  asso=st_nearest_feature(st_transform(outlets,4326),emu)
  outlets$emu=emu$emu_nameshort[asso]
  allcatchments$emu=outlets$emu[match(allcatchments$ser_ccm_wso_id,outlets$ser_ccm_wso_id)]
  
  allcatchments$zone=allcatchments$emu
  
  allcatchments=subset(allcatchments,!is.na(allcatchments$zone)) |>
    filter(zone %in% c("FR_Adou","FR_Garo", "FR_Loir","FR_Bret"))
  allcatchments$zone=as.factor(allcatchments$zone)
  
  zone=aggregate(allcatchments$area_km2,list(allcatchments$zone),sum)
  names(zone)=c("zone","surface")
}
```


## Zone definition

We used the same zones as @bornarel2018a \@ref(fig:mapsZones):

* a North Sea zone (NS)
* a Channel zone which covers Southwestern Great Britanny and NorthWestern France
* ATL_F which covers the French coast along the Bay of Biscay
* ATL_IB which extends from the Cantabrian Sea to the Gibraltar Strait
* Med which extends from the Gibraltar Strait to Sicilia
* A zone that covers Ireland and the Northwestern part of Great Britain (INWGB)


```{r mapsZones, echo=FALSE,warning=FALSE,message=FALSE, fig.height=16/2.54, fig.cap="Zone definition and available data"}
sf::sf_use_s2(FALSE)
worldmap <- ne_countries(scale = 'medium', type = 'map_units',
                         returnclass = 'sf')
europe_cropped <- st_crop(worldmap, xmin = -13, xmax = 27,
                          ymin = 35, ymax = 65)
colpal=iwanthue(length(levels(allcatchments$zone)))
series$jitx=jitter(series$ser_x,amount=.5)
series$jity=jitter(series$ser_y,amount=.5)
ggplot(europe_cropped)+geom_sf(data=europe_cropped,
                               fill="white")+
  
  geom_sf(data=allcatchments,aes(fill=zone),cex=2,col=NA,alpha=1)+
  geom_point(data=series,aes(x=jitx,
                             y=jity,
                             col=type),cex=2,shape=15)+
  geom_point(data=series,aes(x=jitx,
                             y=jity),
             cex=2,shape=0)+
  scale_fill_manual("Zone",values=colorpalette)+
  scale_color_manual("Type of series",values=rev(colorpalette))+
  xlab("")+
  ylab("")+
  theme_bw()
```



<!--
## Modification in the model
In first versions of GEREM, river recruitment in a river basin was assumed to be a deterministic proportion of the corresponding zone recruitment, with the proportions equal to a simple function of the river basin area $S_{c,z}$ to mimic a multinomial distribution (equation \@ref(eq:oldweight)):

$$ 
\begin{equation} 
\begin{aligned}
& R_{c,z}(y) \sim N \left( {R_z(y) \cdot w_{c,z},R_z (y) \cdot w_{c,z}  \cdot (1-w_{c,z}) }\right) \\
& \text{with } w_{c,z} = \frac{S_{c,z}^\beta}{\sum\limits_{b \in z} S_{b,z}^\beta}
\end{aligned}
(\#eq:oldweight)
\end{equation} 
$$
Here, we slightly modified this relationship to account for local heterogeneity among river basins. More specifically, we incorporated a random effect on weights (equation \@ref(eq:newweight)):

$$ 
\begin{equation} 
\begin{aligned}
& w_{c,z} = \frac{S_{c,z}^\beta \cdot e^{\varepsilon(c,z)}}{\sum\limits_{b \in z} S_{b,z}^\beta \cdot e^{\varepsilon(b,z)}} \\
& \text{with } \varepsilon(b,z) \sim N\left(-\frac{1}{2} \cdot \sigma, \sigma^2\right)
\end{aligned}
(\#eq:newweight)
\end{equation} 
$$
-->

## Available Data
Table \@ref(tab:availableData) summarises the data used to fit the model. Basically, we used the exact same dataset as for the GLM analysis. This includes the 4 newly integrated time series: MondG (PT), ShiMG (GB), OatGY(GB) and SousGY. As last, year, the dataset was supplemented with  some absolutes estimates of recruitment following @ices2020a. While time series are available in all zones, most absolute estimates come from ATL_F. In other zones, trap monitoring and commercial catches can inform on absolute estimates given but this requires making assumption on trapping efficiency or on exploitation rates. We also note that the number of time series is limited in the Channel area. Conversely, there are many time series in ATL_F, but most of them ended after the implementation of the French Eel Management Plan [@ministeredelecologiedelenergiedudeveloppementdurableetdelamenagementduterritoire2010] and presently, there is only one still updated time series. We also note that the Mediterranean zone is large with only four available time series.

```{r availableData, echo=FALSE,tab.cap="Available time series of recruitment"}
ft <- flextable(series_stat[order(series_stat$zone,series_stat$ser_nameshort),]) %>%
  colformat_num(j="surface") %>%
  set_header_labels(ser_nameshort="Series",
                    type="Type",
                    zone="Zone",
                    surface="Surface (km²)",
                    first_year="First Year",
                    last_year="Last Year",
                    nbyear="Nb data")%>%
  bold(bold = TRUE, part = "header")
autofit(ft)
```

<br> 
Available time series are assumed to be proportional to real abundance in the river basin with a scaling factor constant through time (otherwise the time series would not be a recruitment abundance index). For absolute estimates, this scaling factor is set to 1 by definition (e.g. absolute estimates provide direct estimates of real abundance in average).
For traps, we use vague priors on trap efficiency to give an insight on the possible recruitment (Figure \@ref(fig:priorsScaling)), we used a vague prior between 0 and 0.35. Indeed, fishway passabilities are often estimated around 1/3 [@briand2005a; @drouineau2015; @jessop2000; @noonan2012], therefore our prior assumes that the observed abundance, corrected for the passability (e.g. multiplied by 3) is a minimum bound for the overall recruitment. For commercial time series, the scaling factor corresponds to the exploitation rate and we used a uniform prior between 0 and 1 (e.g. commercial catch is a minimum value for recruitment), except for the Somme River, in which, based on expert knowledge and following @bornarel2018a, we assumed a large exploitation rate.

```{r priorsScaling, fig.cap="Priors for exploitation rates and trap efficiency. Exploitation rate and trap efficiency make make the link between observed data and models predictions of absolute recruitments", echo=FALSE, warning=FALSE, message=FALSE}
priors=na.omit(unique(series[,c("type","scale_bound_shape1","scale_bound_shape2","min_bound","max_bound")]))
priors=expand_grid(priors,x=seq(0,1,.01))
priors <- priors %>%
  mutate(prior=dbeta(x,scale_bound_shape1,scale_bound_shape2)) %>%
  mutate(prior=ifelse(x<=min_bound | x>=max_bound ,0,prior))
priors$series=mapply(function(t,s1,s2) 
  paste(series$ser_nameshort[series$type==t &
                               series$scale_bound_shape1==s1 &
                               series$scale_bound_shape2==s2],
        collapse="\n"),
  priors$type,priors$scale_bound_shape1,priors$scale_bound_shape2)
ggplot(priors,aes(x=x, y=prior))+
  geom_line(aes(col=series))+
  xlab("scaling factor") + ylab("prior density")+
  scale_color_manual("factor",values=colorpalette)+
  guides(col = guide_legend(ncol = 3))+
  theme_bw()
```

## Running the model
```{r runningConfig, echo=FALSE, include=FALSE}
if ((!file.exists(paste("datagerem_nimble2_",CY,".rdata",sep=""))) ){
  
  burnin=100000
  sample=1000
  thin=50
  
  
  
  row.names(values_wide)=values_wide$das_year
  values_wide=values_wide[,-1]
  
  
  ###############formatting data and inputs
  
  
  nbyear=nrow(values_wide)
  absolute=subset(values_wide,select=which(series$type[match(names(values_wide),series$ser_nameshort)]=="absolute"))
  serie=subset(values_wide,select=which(series$type[match(names(values_wide),series$ser_nameshort)]=="relative"))
  trap=subset(values_wide,select=which(series$type[match(names(values_wide),series$ser_nameshort)]=="trap"))
  catch=subset(values_wide,select=which(series$type[match(names(values_wide),series$ser_nameshort)]=="catch"))
  
  
  #serie=serie+1
  serie=sweep(serie,2, colMeans(serie,na.rm=TRUE),"/")
  logIAObs=as.matrix(log(serie))
  logIAObs[is.infinite(logIAObs)]=NA #we removed 0 
  logUObs=log(absolute)
  logIPObs=as.matrix(log(trap))
  logIPObs[is.infinite(logIPObs)]=NA #we removed 0 
  logIEObs=log(catch)
  
  nbsurvey=ncol(serie)
  nbabsolute=ncol(absolute)
  nbtrap=ncol(trap)
  nbcatch=ncol(catch)
  
  ########formatting catchments
  nbzone=nrow(zone)
  tab_series=unique(series[,c("ser_ccm_wso_id","surface","zone")]) # a table with one row per catchment in which we have data
  
  ###############creating vector of indices to match the different dataset
  catchment_survey=match(series$ser_ccm_wso_id[match(names(serie),series$ser_nameshort)],tab_series$ser_ccm_wso_id)
  catchment_absolute=match(series$ser_ccm_wso_id[match(names(absolute),series$ser_nameshort)],tab_series$ser_ccm_wso_id)
  catchment_trap=match(series$ser_ccm_wso_id[match(names(trap),series$ser_nameshort)],tab_series$ser_ccm_wso_id)
  catchment_catch=match(series$ser_ccm_wso_id[match(names(catch),series$ser_nameshort)],tab_series$ser_ccm_wso_id)
  
  meanlogq=rep(log(.5),ncol(serie))
  
  
  
  # mulogRglobal1=log(sum(colMeans(absolute,na.rm=TRUE)))+log(sum(surfaceZone)/sum(surface[catchment_absolute]))
  
  
  
  save(list=setdiff(ls(),c("cred","cred_ccm")), file = paste("datagerem_nimble2_",CY,".rdata",sep=""))
  dbDisconnect(con_wgeel)
  dbDisconnect(con_ccm)
} else{
  load(paste("datagerem_nimble2_",CY,".rdata",sep=""))
}  


initpropR=rep(1,nbzone)
zone_param <- lapply(seq_len(nrow(zone)), function(iz){
  zonename=zone$zone[iz]
  create_zone(zone$zone[iz],iz,tab_series%>%filter(zone==zonename))
})
myconstants = list(
  nbzone=nbzone,
  nbsurvey=nbsurvey,
  nbtrap=nbtrap,
  nbabsolute=nbabsolute,
  nbcatch=nbcatch,
  catchment_survey=catchment_survey,
  catchment_trap=catchment_trap,
  catchment_catch=catchment_catch,
  catchment_absolute=catchment_absolute,
  initpropR=initpropR,
  nbyear=nbyear,
  scale_trap=sapply(1:nbtrap,function(i){
    as.matrix(series[series$ser_nameshort==names(trap)[i],
                     c("scale_bound_shape1",
                       "scale_bound_shape2")])
  }),
  scale_catch=sapply(1:nbcatch,function(i){
    as.matrix(series[series$ser_nameshort==names(catch)[i],
                     c("scale_bound_shape1",
                       "scale_bound_shape2")])
  }),
  min_trap=sapply(1:nbtrap,function(i){
    series[series$ser_nameshort==names(trap)[i],"min_bound"]
  }),
  max_trap=sapply(1:nbtrap,function(i){
    series[series$ser_nameshort==names(trap)[i],"max_bound"]
  }),
  min_catch=sapply(1:nbcatch,function(i){
    series[series$ser_nameshort==names(catch)[i],"min_bound"]
  }),
  max_catch=sapply(1:nbcatch,function(i){
    series[series$ser_nameshort==names(catch)[i],"max_bound"]
  }))

for(z in zone_param){
  myconstants <- c(myconstants,z[-1])
}
code <- paste(sapply(zone_param, function(x) x[["code"]]),collapse='\n')
code <- paste(code, "concentration_zone~dunif(10,100)", sep="\n\n")

readLines("modelCode_nimble2.R") %>%
  stringr::str_replace("RCM",code) %>%
  writeLines("modelCode_nimblerevised.R")



mydata=list(
  logIAObs=as.matrix(logIAObs),
  logIPObs=as.matrix(logIPObs),
  logUObs=as.matrix(logUObs),
  logIEObs=as.matrix(logIEObs)
)

generate_init=function(){
  gen_init=function(x){
    logIAObs=apply(mydata$logIAObs,2,function(x){
      d=which(!is.na(x))
      x[which(is.na(x))]=runif(length(which(is.na(x))),min(x,na.rm = TRUE)*2,max(x,na.rm = TRUE)*2)
      x[d]=NA
      x
    })
    logIPObs=apply(mydata$logIPObs,2,function(x){
      d=which(!is.na(x))
      x[which(is.na(x))]=runif(length(which(is.na(x))),min(x,na.rm = TRUE)*2,max(x,na.rm = TRUE)*2)
      x[d]=NA
      x
    })
    
    logUObs=apply(mydata$logUObs,2,function(x){
      d=which(!is.na(x))
      x[which(is.na(x))]=runif(length(which(is.na(x))),min(x,na.rm = TRUE)*2,max(x,na.rm = TRUE)*2)
      x[d]=NA
      x
    })
    logIEObs=apply(mydata$logIEObs,2,function(x){
      d=which(!is.na(x))
      x[which(is.na(x))]=runif(length(which(is.na(x))),min(x,na.rm = TRUE)*2,max(x,na.rm = TRUE)*2)
      x[d]=NA
      x
    })
    
    
    #inits
    propR=matrix(0,myconstants$nbzone,myconstants$nbyear)
    for (i in 1:myconstants$nbyear){
      tmp=rbeta(myconstants$nbzone,1,1)
      propR[,i]=tmp/sum(tmp)
    }
    
    sdq=(runif(1,0.26,1))
    sdRglob=(runif(1,0.26,1))
    sdRwalk=(runif(1,0.26,1))
    sdIA=(runif(myconstants$nbsurvey,0.26,1))
    sdIP=(runif(myconstants$nbtrap,0.26,1))
    sdU=(runif(myconstants$nbabsolute,0.26,1))
    sdIE=(runif(myconstants$nbcatch,0.26,1))
    
    epsilonRzone=rnorm(myconstants$nbyear*myconstants$nbzone,0,1)
    epsilonR=rnorm(myconstants$nbyear,0,1)
    beta=runif(1,.75,.82)
    logR1=runif(1,6,15)
    logq=runif(ncol(mydata$logIAObs),-13,0)
    a=mapply(function(mi,ma) runif(1,mi,ma), myconstants$min_trap,myconstants$max_trap)
    # VZ <- sapply(seq_len(myconstants$nbzone), function(z){
    #   log((exp(1/tauLocal)-1)*(sum(pow(pow(myconstants$surfaceallcatchment[z,seq_len(myconstants$nbcatchmentzone[z])],beta)*exp(-0.5/tauLocal),2))/pow(sum(pow(myconstants$surfaceallcatchment[z,seq_len(myconstants$nbcatchmentzone[z])],beta)*exp(-0.5/tauLocal)),2))+1)})
    # 
    # 
    # 
    # muWeightZone <- sapply(seq_len(myconstants$nbzone), function(z){   log(sum(pow(myconstants$surfaceallcatchment[z,seq_len(myconstants$nbcatchmentzone[z])],beta)*exp(-0.5/tauLocal)))+0.5/tauLocal-0.5*VZ[z]^2})
    # weightZone <- mapply(function(mu,sd) rlnorm(1,mu,sd),muWeightZone,sqrt(VZ))
    # 
    
    p=mapply(function(mi,ma) runif(1,mi,ma), myconstants$min_catch,myconstants$max_catch)
    inits=list(sdIE=sdIE,sdq=sdq,propR=propR,sdRglob=sdRglob,
               sdIA=sdIA,sdIP=sdIP,sdU=sdU, #precisionpropRwalk=precisionpropRwalk,      
               epsilonRzone=epsilonRzone,epsilonR=epsilonR,
               beta=beta,
               # weightZone=weightZone,
               logR1=logR1,logIAObs=logIAObs,logIPObs=logIPObs,logUObs=logUObs,
               logIEObs=logIEObs,sdRwalk=sdRwalk,
               logq=logq,a=a,p=p)
    inits
  }
  gen_init(1)
}

```
Three independent MCMC chains are run in parallel using JAGS [@plummer2003] through R package runjags [@denwood2016]. Chains were run `r format(sample*thin, scientific=FALSE)` iterations, with a thinning of `r format(thin,scientif=FALSE)` iterations, after an initial burnin period of `r format(burnin, scientific=FALSE)` iterations. Gelman and Rubin diagnostics were used to check model convergence [@gelman1992].

```{r Load samOptiPro package}
devtools::load_all("~/samOptiPro_packages_dev/samOptiPro")
devtools::document()
devtools::load_all()
```

```{r modelRun,echo=FALSE,message=FALSE,warning=FALSE}
if  ((!file.exists(paste("gerem_nimble2_",CY,".rdata",sep=""))) ){
  
  library(parallel)
  cl <- makeCluster(3)
  load(paste0("datagerem_nimble2_",CY,".rdata"))
  clusterExport(cl, c("myconstants","mydata","generate_init", "burnin", "sample", "thin"))
  res <- parLapply(cl, 1:3, function(seed){
    library(nimble)
    source("modelCode_nimblerevised.R")

    geremModel <- nimbleModel(
      code = geremCode,
      constants=myconstants,
      data=mydata,
      inits=generate_init(),
      calculate = FALSE)
   
  
    geremconfMCMC <- configureMCMC(geremModel,
                                   monitors=c("beta","logq","loga",
                                              "logRglobal","Rzone","propR","logp",
                                              "sdIA","sdIP","Rcm","sdIE","sdU", "concentration_zone"))
    
    geremMCMC <- buildMCMC(
      geremconfMCMC,
      niter=(sample + burnin) * thin,
      thin=thin,
      nburnin=burnin,
      chain=1)
     timeStart_compile <- Sys.time()  # Start timer for compilation

    Cmodel <- compileNimble(geremModel)
    
    compiledgerem<-compileNimble(geremMCMC, project=geremModel)
    
 
    
    test <- runMCMC(compiledgerem, niter=burnin + (sample) * thin,
                    thin=50,
                    nburnin=burnin, nchains=1, setSeed = seed)
    test
    
  })
  
  
  jags_res <- mcmc.list(lapply(res, function(i) mcmc(i)))
  jags_mat <- as.matrix(jags_res)
  colname <- colnames(jags_mat)
  save(jags_res,jags_mat,colname,file=paste("gerem_nimble2",CY,".rdata",sep=""))
  
  
  
  
} else {
  load(paste("gerem2",CY,".rdata",sep=""))
}
```

```{r GEREM Model_origonal}
geremCode <- nimbleCode({
  #####Zone 1

weights_z1[1:(1+length(surface_catchments_zone_series_z1))] <- c(pow(surface_catchments_zone_series_z1[1:(length(surface_catchments_zone_series_z1))],beta), sum(pow(surface_catchments_not_in_series_z1[1:(length(surface_catchments_not_in_series_z1))],beta)))

alpha_z1[1:(1+length(surface_catchments_zone_series_z1))] <- weights_z1[1:(1+length(surface_catchments_zone_series_z1))] / sum(weights_z1[1:(1+length(surface_catchments_zone_series_z1))]) * concentration_zone

for (y in 1:nbyear){
  props_z1[y,1:(1+length(surface_catchments_zone_series_z1))] ~ ddirich(alpha_z1[1:(1+length(surface_catchments_zone_series_z1))])
}

for (y in 1:nbyear){
    Rcm[y,1] <- props_z1[y,1]*Rzone[y,1]
    Rcm[y,8] <- props_z1[y,2]*Rzone[y,1]
}
#####Zone 2

weights_z2[1:2] <- c(pow(surface_catchments_zone_series_z2,beta), sum(pow(surface_catchments_not_in_series_z2[1:(length(surface_catchments_not_in_series_z2))],beta)))

alpha_z2[1:2] <- weights_z2[1:2] / sum(weights_z2[1:2]) * concentration_zone

for (y in 1:nbyear){
  props_z2[y,1:2] ~ ddirich(alpha_z2[1:2])
}

for (y in 1:nbyear){
    Rcm[y,9] <- props_z2[y,1]*Rzone[y,2]
}
#####Zone 3

weights_z3[1:(1+length(surface_catchments_zone_series_z3))] <- c(pow(surface_catchments_zone_series_z3[1:(length(surface_catchments_zone_series_z3))],beta), sum(pow(surface_catchments_not_in_series_z3[1:(length(surface_catchments_not_in_series_z3))],beta)))

alpha_z3[1:(1+length(surface_catchments_zone_series_z3))] <- weights_z3[1:(1+length(surface_catchments_zone_series_z3))] / sum(weights_z3[1:(1+length(surface_catchments_zone_series_z3))]) * concentration_zone

for (y in 1:nbyear){
  props_z3[y,1:(1+length(surface_catchments_zone_series_z3))] ~ ddirich(alpha_z3[1:(1+length(surface_catchments_zone_series_z3))])
}

for (y in 1:nbyear){
    Rcm[y,2] <- props_z3[y,1]*Rzone[y,3]
    Rcm[y,3] <- props_z3[y,4]*Rzone[y,3]+props_z3[y,3]*Rzone[y,3]
    Rcm[y,4] <- props_z3[y,4]*Rzone[y,3]+props_z3[y,3]*Rzone[y,3]
    Rcm[y,6] <- props_z3[y,2]*Rzone[y,3]
}
#####Zone 4

weights_z4[1:(1+length(surface_catchments_zone_series_z4))] <- c(pow(surface_catchments_zone_series_z4[1:(length(surface_catchments_zone_series_z4))],beta), sum(pow(surface_catchments_not_in_series_z4[1:(length(surface_catchments_not_in_series_z4))],beta)))

alpha_z4[1:(1+length(surface_catchments_zone_series_z4))] <- weights_z4[1:(1+length(surface_catchments_zone_series_z4))] / sum(weights_z4[1:(1+length(surface_catchments_zone_series_z4))]) * concentration_zone

for (y in 1:nbyear){
  props_z4[y,1:(1+length(surface_catchments_zone_series_z4))] ~ ddirich(alpha_z4[1:(1+length(surface_catchments_zone_series_z4))])
}

for (y in 1:nbyear){
    Rcm[y,5] <- props_z4[y,1]*Rzone[y,4]
    Rcm[y,7] <- props_z4[y,2]*Rzone[y,4]
}

concentration_zone~dunif(10,100)
  
  #prior for first year of recruitment
  logRglobal[1]<-logR1
  Rglobal[1]<-exp(logRglobal[1])
  
  for (zone in 1:(nbzone)){
    Rzone[1,zone]<-Rglobal[1]*propR[zone,1]
  }
  
  for (y in 2:nbyear){
    mulogRglobal[y]<-logRglobal[y-1]
    logRglobal[y]<-mulogRglobal[y]+epsilonR[y]*sdRwalk
    Rglobal[y]<-exp(logRglobal[y])
    for (zone in 1:(nbzone)){
      #		Rzonepred[y,zone]<-Rglobal[y]*propR[zone]
      #		sdRzone[y,zone]<-sqrt((0.000001+Rglobal[y])*(0.000001+propR[zone])*(.999999999999-propR[zone]))
      #		Rzone[y,zone]<-max(0.000000001,Rzonepred[y,zone]+epsilonRzone[(y-1)*nbzone+zone]*sdRzone[y,zone]) #troncature pour éviter des log de 0
      Rzone[y,zone]<-propR[zone,y]*Rglobal[y]
    }
  }
  for (zone in 1:nbzone){
    Rzone_final[zone]<-Rzone[nbyear,zone]
    alpha[zone]<-initpropR[zone]+0.01#*precisionpropRwalk
  }
  
  
  for (y in 1:nbyear){
    if (nbsurvey > 1){
      for (isurvey in 1:nbsurvey){
        ########on doit connaitre: capturabilité du survey q[survey], la surface du bv bvsurvey[survey]
        ########  
        logIApred[y,isurvey]<-logq[isurvey]+log(Rcm[y,catchment_survey[isurvey]])-0.5/tauIA[isurvey]
        logIAObs[y,isurvey]~dnorm(logIApred[y,isurvey],tauIA[isurvey])
      }
    } else {
      logIApred[y, 1]<-logq[1]+log(Rcm[y,catchment_survey])-0.5/tauIA[1]
      logIAObs[y, 1]~dnorm(logIApred[y],tauIA[1])
    }
    if (nbtrap > 1){
      for (itrap in 1:nbtrap){
        ########on doit connaître l'efficacité du piège
        logIPpred[y,itrap]<-loga[itrap]+log(Rcm[y,catchment_trap[itrap]])-0.5/tauIP[itrap]
        logIPObs[y,itrap]~dnorm(logIPpred[y,itrap],tauIP[itrap])
      }
    } else {
      logIPpred[y, 1]<-loga[1]+log(Rcm[y,catchment_trap])-0.5/tauIP[1]
      logIPObs[y, 1]~dnorm(logIPpred[y, 1],tauIP[1])
    }
    if (nbcatch > 1){
      for (icatch in 1:nbcatch){
        ########on doit connaître l'efficacité du piège
        logIEpred[y,icatch]<-logp[icatch]+log(Rcm[y,catchment_catch[icatch]])-0.5/tauIE[icatch]
        logIEObs[y,icatch]~dnorm(logIEpred[y,icatch],tauIE[icatch])
      }
    } else {
      logIEpred[y, 1]<-logp[1]+log(Rcm[y,catchment_catch])-0.5/tauIE[1]
      logIEObs[y,1]~dnorm(logIEpred[y, 1],tauIE[1])
    }
    if (nbabsolute > 1) {
      for (iabsolute in 1:nbabsolute){
        ########on doit connaitre: pareil qu avant mais sans capturabilité
        logUpred[y,iabsolute]<-log(Rcm[y,catchment_absolute[iabsolute]])-0.5/tauU[iabsolute]
        logUObs[y,iabsolute]~dnorm(logUpred[y,iabsolute],tauU[iabsolute])
      }
    } else {
      logUpred[y]<-log(Rcm[y,catchment_absolute])-0.5/tauU[1]
      logUObs[y,1]~dnorm(logUpred[y],tauU[1])
    }
  }
  beta~dunif(0.71,1.3)
  propR[1:nbzone,1]~ddirich(alpha[1:nbzone])
  for (y in 2:nbyear){
    alphaYear[y-1, 1:nbzone] <- lambda*propR[1:nbzone,y-1]
    propR[1:nbzone,y]~ddirich(alphaYear[y-1, 1:nbzone])
  }
  lambda<-80
  
  
  for (y in 1:(nbzone*nbyear)){
    epsilonRzone[y]~dnorm(0,1)
  }
  for (y in 1:nbyear){
    epsilonR[y]~dnorm(0,1)
  }
  logR1~dunif(6,15)
  for (survey in 1:nbsurvey){
    logq[survey]~dunif(-13,0)
  }
  if (nbtrap > 1){
    for (itrap in 1:nbtrap){
      a[itrap]~T(dbeta(scale_trap[1,itrap],scale_trap[2,itrap]),min_trap[itrap],max_trap[itrap])
      loga[itrap]<-log(a[itrap])
    }
  } else {
    a[1]~T(dbeta(scale_trap[1,1],scale_trap[2,1]),min_trap,max_trap)
    loga[1]<-log(a[1])
  }
  if (nbcatch > 1){
    for (icatch in 1:nbcatch){
      p[icatch]~T(dbeta(scale_catch[1,icatch],scale_catch[2,icatch]),min_catch[icatch],max_catch[icatch])
      logp[icatch]<-log(p[icatch]) 
    }
  } else {
    p[1]~T(dbeta(scale_catch[1,1],scale_catch[2,1]),min_catch,max_catch)
    logp[1]<-log(p[1]) 
  }
  
  #precisionpropRwalk~dunif(0.5,1)
  
  
  
  sdRglob ~ T(dt(df=1,mu=0,sigma=2.5),0.001,1)
  tauRglob <- 1/sdRglob^2
  
  
  for (isurvey in 1:nbsurvey){
    tauIA[isurvey] <- sdIA[isurvey]
    sdIA[isurvey]~ T(dt(df=1,mu=0,sigma=2.5),0.001,1)
  }
  for (itrap in 1:nbtrap){
    tauIP[itrap] <- 1/sdIP[itrap]^2
    sdIP[itrap]~T(dt(df=1,mu=0,sigma=2.5),0.001,1)
  }
  
  for (iabsolute in 1:nbabsolute){
    tauU[iabsolute] <- 1/sdU[iabsolute]^2
    sdU[iabsolute]~T(dt(df=1,mu=0,sigma=2.5),0.001,1)
  }
  for (icatch in 1:nbcatch){
    tauIE[icatch]<-1/sdIE[icatch]^2
    sdIE[icatch]~T(dt(df=1,mu=0,sigma=2.5),0.001,1)
  }
  
  sdq ~T(dt(df=1,mu=0,sigma=2.5),0.001,1)
  tauq <- 1/sdq^2
  sdRwalk ~ T(dt(df=1,mu=0,sigma=2.5),0.001,1)
  tauRwalk <- 1/sdRwalk^2
})

```

```{r GEREM Model}
geremCode <- nimbleCode({
  #####Zone 1

  weights_z1[1:(1+length(surface_catchments_zone_series_z1))] <- c(
    pow(surface_catchments_zone_series_z1[1:(length(surface_catchments_zone_series_z1))], beta),
    sum(pow(surface_catchments_not_in_series_z1[1:(length(surface_catchments_not_in_series_z1))], beta))
  )

  alpha_z1[1:(1+length(surface_catchments_zone_series_z1))] <-
    weights_z1[1:(1+length(surface_catchments_zone_series_z1))] /
    sum(weights_z1[1:(1+length(surface_catchments_zone_series_z1))]) * concentration_zone

  for (y in 1:nbyear){
    props_z1[y,1:(1+length(surface_catchments_zone_series_z1))] ~
      ddirich(alpha_z1[1:(1+length(surface_catchments_zone_series_z1))])
  }

  for (y in 1:nbyear){
    Rcm[y,1] <- props_z1[y,1]*Rzone[y,1]
    Rcm[y,8] <- props_z1[y,2]*Rzone[y,1]
  }

  #####Zone 2

  weights_z2[1:2] <- c(
    pow(surface_catchments_zone_series_z2,beta),
    sum(pow(surface_catchments_not_in_series_z2[1:(length(surface_catchments_not_in_series_z2))],beta))
  )

  alpha_z2[1:2] <- weights_z2[1:2] / sum(weights_z2[1:2]) * concentration_zone

  for (y in 1:nbyear){
    props_z2[y,1:2] ~ ddirich(alpha_z2[1:2])
  }

  for (y in 1:nbyear){
    Rcm[y,9] <- props_z2[y,1]*Rzone[y,2]
  }

  #####Zone 3

  weights_z3[1:(1+length(surface_catchments_zone_series_z3))] <- c(
    pow(surface_catchments_zone_series_z3[1:(length(surface_catchments_zone_series_z3))],beta),
    sum(pow(surface_catchments_not_in_series_z3[1:(length(surface_catchments_not_in_series_z3))],beta))
  )

  alpha_z3[1:(1+length(surface_catchments_zone_series_z3))] <-
    weights_z3[1:(1+length(surface_catchments_zone_series_z3))] /
    sum(weights_z3[1:(1+length(surface_catchments_zone_series_z3))]) * concentration_zone

  for (y in 1:nbyear){
    props_z3[y,1:(1+length(surface_catchments_zone_series_z3))] ~
      ddirich(alpha_z3[1:(1+length(surface_catchments_zone_series_z3))])
  }

  for (y in 1:nbyear){
    Rcm[y,2] <- props_z3[y,1]*Rzone[y,3]
    Rcm[y,3] <- props_z3[y,4]*Rzone[y,3]+props_z3[y,3]*Rzone[y,3]
    Rcm[y,4] <- props_z3[y,4]*Rzone[y,3]+props_z3[y,3]*Rzone[y,3]
    Rcm[y,6] <- props_z3[y,2]*Rzone[y,3]
  }

  #####Zone 4

  weights_z4[1:(1+length(surface_catchments_zone_series_z4))] <- c(
    pow(surface_catchments_zone_series_z4[1:(length(surface_catchments_zone_series_z4))],beta),
    sum(pow(surface_catchments_not_in_series_z4[1:(length(surface_catchments_not_in_series_z4))],beta))
  )

  alpha_z4[1:(1+length(surface_catchments_zone_series_z4))] <-
    weights_z4[1:(1+length(surface_catchments_zone_series_z4))] /
    sum(weights_z4[1:(1+length(surface_catchments_zone_series_z4))]) * concentration_zone

  for (y in 1:nbyear){
    props_z4[y,1:(1+length(surface_catchments_zone_series_z4))] ~
      ddirich(alpha_z4[1:(1+length(surface_catchments_zone_series_z4))])
  }

  for (y in 1:nbyear){
    Rcm[y,5] <- props_z4[y,1]*Rzone[y,4]
    Rcm[y,7] <- props_z4[y,2]*Rzone[y,4]
  }

  concentration_zone~dunif(10,100)

  #prior for first year of recruitment
  logRglobal[1]<-logR1
  Rglobal[1]<-exp(logRglobal[1])

  for (zone in 1:(nbzone)){
    Rzone[1,zone]<-Rglobal[1]*propR[zone,1]
  }

  for (y in 2:nbyear){
    mulogRglobal[y]<-logRglobal[y-1]
    logRglobal[y]<-mulogRglobal[y]+epsilonR[y]*sdRwalk
    Rglobal[y]<-exp(logRglobal[y])
    for (zone in 1:(nbzone)){
      Rzone[y,zone]<-propR[zone,y]*Rglobal[y]
    }
  }

  for (zone in 1:nbzone){
    Rzone_final[zone]<-Rzone[nbyear,zone]
    alpha[zone]<-initpropR[zone]+0.01
  }

  for (y in 1:nbyear){
    if (nbsurvey > 1){
      for (isurvey in 1:nbsurvey){
        ########on doit connaitre: capturabilité du survey q[survey], la surface du bv bvsurvey[survey]
        logIApred[y,isurvey]<-logq[isurvey]+log(Rcm[y,catchment_survey[isurvey]])-0.5/tauIA[isurvey]
        logIAObs[y,isurvey]~dnorm(logIApred[y,isurvey],tauIA[isurvey])
      }
    } else {
      logIApred[y, 1]<-logq[1]+log(Rcm[y,catchment_survey])-0.5/tauIA[1]
      logIAObs[y, 1]~dnorm(logIApred[y],tauIA[1])
    }
    if (nbtrap > 1){
      for (itrap in 1:nbtrap){
        ########on doit connaître l'efficacité du piège
        logIPpred[y,itrap]<-loga[itrap]+log(Rcm[y,catchment_trap[itrap]])-0.5/tauIP[itrap]
        logIPObs[y,itrap]~dnorm(logIPpred[y,itrap],tauIP[itrap])
      }
    } else {
      logIPpred[y, 1]<-loga[1]+log(Rcm[y,catchment_trap])-0.5/tauIP[1]
      logIPObs[y, 1]~dnorm(logIPpred[y, 1],tauIP[1])
    }
    if (nbcatch > 1){
      for (icatch in 1:nbcatch){
        ########on doit connaître l'efficacité du piège
        logIEpred[y,icatch]<-logp[icatch]+log(Rcm[y,catchment_catch[icatch]])-0.5/tauIE[icatch]
        logIEObs[y,icatch]~dnorm(logIEpred[y,icatch],tauIE[icatch])
      }
    } else {
      logIEpred[y, 1]<-logp[1]+log(Rcm[y,catchment_catch])-0.5/tauIE[1]
      logIEObs[y,1]~dnorm(logIEpred[y, 1],tauIE[1])
    }
    if (nbabsolute > 1) {
      for (iabsolute in 1:nbabsolute){
        ########on doit connaitre: pareil qu avant mais sans capturabilité
        logUpred[y,iabsolute]<-log(Rcm[y,catchment_absolute[iabsolute]])-0.5/tauU[iabsolute]
        logUObs[y,iabsolute]~dnorm(logUpred[y,iabsolute],tauU[iabsolute])
      }
    } else {
      logUpred[y]<-log(Rcm[y,catchment_absolute])-0.5/tauU[1]
      logUObs[y,1]~dnorm(logUpred[y],tauU[1])
    }
  }

  beta~dunif(0.71,1.3)
  propR[1:nbzone,1]~ddirich(alpha[1:nbzone])
  for (y in 2:nbyear){
    alphaYear[y-1, 1:nbzone] <- lambda*propR[1:nbzone,y-1]
    propR[1:nbzone,y]~ddirich(alphaYear[y-1, 1:nbzone])
  }
  lambda<-80

  for (y in 1:(nbzone*nbyear)){
    epsilonRzone[y]~dnorm(0,1)
  }
  for (y in 1:nbyear){
    epsilonR[y]~dnorm(0,1)
  }

  logR1~dunif(6,15)

  for (survey in 1:nbsurvey){
    logq[survey]~dunif(-13,0)
  }

  ## --- Betas TRONQUÉES (je les laisse telles quelles, tu pourras
  ##     éventuellement les passer aussi en "soft" si tu crées dbeta_soft) ---

  if (nbtrap > 1){
    for (itrap in 1:nbtrap){
      a[itrap]~T(dbeta(scale_trap[1,itrap],scale_trap[2,itrap]),
                 min_trap[itrap],max_trap[itrap])
      loga[itrap]<-log(a[itrap])
    }
  } else {
    a[1]~T(dbeta(scale_trap[1,1],scale_trap[2,1]),min_trap,max_trap)
    loga[1]<-log(a[1])
  }

  if (nbcatch > 1){
    for (icatch in 1:nbcatch){
      p[icatch]~T(dbeta(scale_catch[1,icatch],scale_catch[2,icatch]),
                  min_catch[icatch],max_catch[icatch])
      logp[icatch]<-log(p[icatch])
    }
  } else {
    p[1]~T(dbeta(scale_catch[1,1],scale_catch[2,1]),min_catch,max_catch)
    logp[1]<-log(p[1])
  }

  ## --- Priors soft (remplacement des T(dt(df=1, mu=0, sigma=2.5), 0.001, 1)) ---

  ## C’était : sdRglob ~ T(dt(df=1,mu=0,sigma=2.5),0.001,1)
  ## => équivalent à une half-Cauchy(2.5) tronquée.
  ## On la remplace par une half-Cauchy SOFT, scale=2.5, alpha=beta=1.

  sdRglob ~ dhalfcauchy_soft(scale = 2.5, alpha = 1, beta = 1)
  tauRglob <- 1/sdRglob^2

  for (isurvey in 1:nbsurvey){
    tauIA[isurvey] <- sdIA[isurvey]
    sdIA[isurvey] ~ dhalfcauchy_soft(scale = 2.5, alpha = 1, beta = 1)
  }

  for (itrap in 1:nbtrap){
    tauIP[itrap] <- 1/sdIP[itrap]^2
    sdIP[itrap] ~ dhalfcauchy_soft(scale = 2.5, alpha = 1, beta = 1)
  }

  for (iabsolute in 1:nbabsolute){
    tauU[iabsolute] <- 1/sdU[iabsolute]^2
    sdU[iabsolute] ~ dhalfcauchy_soft(scale = 2.5, alpha = 1, beta = 1)
  }

  for (icatch in 1:nbcatch){
    tauIE[icatch]<-1/sdIE[icatch]^2
    sdIE[icatch] ~ dhalfcauchy_soft(scale = 2.5, alpha = 1, beta = 1)
  }

  sdq ~ dhalfcauchy_soft(scale = 2.5, alpha = 1, beta = 1)
  tauq <- 1/sdq^2

  sdRwalk ~ dhalfcauchy_soft(scale = 2.5, alpha = 1, beta = 1)
  tauRwalk <- 1/sdRwalk^2
})
```

```{r Building and compiling}
## ================================================================
##  Build + compile + wrap model (multi-chain compatible version)
## ================================================================
source("soft_priors.R")
geremModel <- nimbleModel(
      code = geremCode,
      constants=myconstants,
      data=mydata,
      inits=generate_init(),
      calculate = FALSE)

nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)
nimbleOptions(MCMCsaveHistory = FALSE)
monitors=c("beta","logq","loga","logRglobal","Rzone","propR","logp","sdIA","sdIP","Rcm","sdIE","sdU", "concentration_zone")

  m <- nimbleModel(
    code       = geremCode,
    name       = sprintf("Gerem"),
    constants  = myconstants,
    data       = mydata,
    inits      =generate_init(),  
    buildDerivs = TRUE                     
  )

  m$initializeInfo()

cm <- compileNimble(m, showCompilerOutput = FALSE)
geremconfMCMC <- configureMCMC(geremModel,monitors=monitors)


```
```{r build}
build_M<-function()list(
    model      = m,
    cmodel     = cm,
    conf       = geremconfMCMC,
    monitors   = monitors,
    code_text  = paste(deparse(geremModel), collapse = "\n"))
```

```{r  Baseline MCMC and Performance Assessment,message=FALSE, warning=FALSE}
## ================================================================
##  Baseline MCMC run with distinct initializations per chain
## ================================================================

n.iter   <- 350000
n.burnin <- 100000
n.thin   <- 50
n.chains <- 3

## --- run baseline with per-chain initialization
res_b <- run_baseline_config(
  build_fn = build_M,          # generic builder (uses chain_id)
  niter    = n.iter,
  nburnin  = n.burnin,
  thin     = n.thin,
  monitors = monitors,
  nchains  = n.chains
  #,inits    = generate_init()   # << each chain uses its own init
)

## --- optional save/load
# saveRDS(res_b, file = "outputs/res_b.rds")
# res_b <- readRDS("outputs/res_b.rds")

## --- extract samples as mcmc.list
samples_ml <- as_mcmc_list_sop(
  res_b$samples,
  res_b$samples2,
  drop_loglik = FALSE,
  thin = n.thin
)

runtime_s <- res_b$runtime_s
#### Others performances tests

#ap  <- assess_performance(samples_ml, runtime_s)
runtime_s
#ap$summary
bot2<-identify_bottlenecks_family(samples_ml, runtime_s,
                                 ess_threshold = 1000,
                                 sampler_params = NULL,
                                 model = m,
                                 mcmc_conf = geremconfMCMC,
                                 ignore_patterns = c("^lifted_","^logProb_"),
                                 strict_sampler_only = TRUE,
                                 auto_configure = TRUE,
                                 rhat_threshold = 1.01,
                                 ess_per_s_min = 0)
bot2$top3
#bot<-identify_bottlenecks(samples_ml, runtime_s,
                                 #ess_threshold = 1000,
                                 #sampler_params = NULL,
                                 #model = m,
                                 #mcmc_conf = NULL,
                                 #ignore_patterns = c("^lifted_","^logProb_"),
                                 #strict_sampler_only = TRUE,
                                 #auto_configure = TRUE,
                                 #rhat_threshold = 1.01,
                                 #ess_per_s_min = 0)
#bot$top3
```


```{r gerem baseline parallel}
library(future.apply)
library(coda)

## --- Paramètres
n.iter   <- 350000
n.burnin <- 100000
n.thin   <- 50
n.chains <- 3

## --- Autoriser de gros objets pour les workers
## ici 8 Go, à ajuster si besoin
options(future.globals.maxSize = 8 * 1024^3)

## --- Plan parallèle (inchangé)
if (.Platform$OS.type == "windows") {
  plan(multisession, workers = n.chains)
} else {
  plan(multicore, workers = n.chains)
}

## --- 1) Une chaîne par worker
run_one_chain <- function(chain_id) {
  ## si tu veux des seeds distincts :
  ## set.seed(1000 + chain_id)

  run_baseline_config(
    build_fn = build_M,        # builder générique
    niter    = n.iter,
    nburnin  = n.burnin,
    thin     = n.thin,
    monitors = monitors,
    nchains  = 1
    
  )
}

res_list <- future_lapply(seq_len(n.chains), run_one_chain)

## --- 2) Fusion en un seul objet hmc_full
samples_list <- lapply(res_list, function(r) {
  s <- r$samples

  if (inherits(s, "mcmc.list")) {
    ## on prend la première chaîne si c'est déjà une liste
    s[[1]]
  } else if (inherits(s, "mcmc")) {
    ## déjà au bon format
    s
  } else {
    ## matrice / data.frame -> mcmc
    coda::mcmc(s)
  }
})

## construire une mcmc.list à partir d’une liste d’objets mcmc
samples_ml <- do.call(coda::mcmc.list, samples_list)

hmc_full <- list(
  samples   = samples_ml,
  runtime_s = sum(
    vapply(res_list, function(r) r$runtime_s, numeric(1)),
    na.rm = TRUE
  )
)

## --- 3) Diagnostics & plots
hmc_full$diag_tbl <- compute_diag_from_mcmc_vect(
  hmc_full$samples,
  runtime_s    = hmc_full$runtime_s,
  compute_rhat = "both",
  ess_for      = "both"
)

```


```{r Graph structure-dependency information,message=FALSE, warning=FALSE}

diag_s <- diagnose_model_structure(model =m,
                                     include_data        = FALSE,
                                     removed_nodes       = NULL,
                                     ignore_patterns     = c("^lifted_", "^logProb_"),
                                     make_plots          = TRUE,
                                     output_dir          ="outputs/diagnostics",
                                     save_csv            = FALSE,
                                     node_of_interest    = NULL,
                                     sampler_times       = TRUE,
                                     sampler_times_unit  = "seconds",
                                     auto_profile        = TRUE,
                                     profile_niter       = 1000,
                                     profile_burnin      = 100,
                                     profile_thin        = 1,
                                     profile_seed        = NULL,
                                     np                  = 0.10,
                                     by_family           = TRUE,
                                     family_stat         = "median",#c("median","mean","sum")
                                     time_normalize      = "none",#c("none","per_node")
                                     only_family_plots   = TRUE)
#diag_s$plots
cat(sprintf("- Stochastic nodes   : %d\n", length(diag_s$stochastic_nodes)))
cat(sprintf("- Deterministic nodes: %d\n", length(diag_s$deterministic_nodes)))
```


```{r Diagnosing Differentiability and HMC Eligibility,message=FALSE, warning=FALSE}
out <- run_structure_and_hmc_test(build_M, try_hmc=FALSE, include_data = FALSE)
out
```



```{r Adaptive Block Strategy — test_strategy_block(),message=FALSE, warning=FALSE}

diff<-test_strategy_family(build_fn = build_M,
                         monitors            = NULL,   # optional, just passed through
                         try_hmc             = TRUE,   # only used for full-model path; surgical ignores
                         nchains             = 10,
                         pilot_niter         = 2200,
                         pilot_burnin        = 100,
                         thin                = 2,
                         out_dir             = "outputs/diagnostics",
                         nbot                = 1,
                         # strict sequences (user can override; order strictly enforced)
                         strict_scalar_seq   = c("NUTS","slice","RW"),
                         strict_block_seq    = c("NUTS_block","AF_slice","RW_block"),
                         # forcing
                         force_families      = NULL,   # e.g. c("logit_theta","N")
                         force_nodes         = NULL,   # e.g. list(logit_theta=c("logit_theta[1]",...))
                         force_union         = NULL,   # e.g. c("logit_theta","N")
                         # interaction
                         ask                 = TRUE,
                         ask_before_hmc      = TRUE,
                         # safety caps
                         block_max           = 20,
                         # sampler controls
                         slice_control       = list(),
                         rw_control          = list(),
                         rwblock_control     = list(adaptScaleOnly = TRUE),
                         af_slice_control    = list(),
                         slice_max_contractions = 5000) 

hmc_full <- configure_hmc_safely_bis(
    build_fn   = build_M,
    niter      = 2200,
    nburnin    = 0,
    thin       = 2,
    nchains    = 10,
    monitors   = monitor,
    nuts_mode  = "all",           # global
    enable_WAIC = exists("enableWAIC"),
    out_dir    = NULL  #"outputs/hmc_full"
)
samples_ml <- hmc_full$samples$samples      # ou ...$samples2
stopifnot(inherits(samples_ml, "mcmc.list"))
diag_tbl <- compute_diag_from_mcmc_vect(
     samples_ml,
     runtime_s = hmc_full$runtime_s,
     compute_rhat = "both",
    ess_for = "both",
     target_block_ram_gb = 2
)
hmc_full$diag_tbl  <- diag_tbl
hmc_full$samples   <- samples_ml


# hmc_full <- test_strategy_family_fast(...)  # ou test_strategy_family(...) # Objet retourne par les fonction test_strategy_family_fast() #or test_strategy_family ou configure_hmc_safely
plot_strategies_from_test_result_fast(
  hmc_full,
  out_dir = "outputs/strategies_plots",
  per     = "family",   # ou "family" ou "target"
  top_k   = 40L,
  top_by  = "AE"        # ou "AE" or "CE"
)

hmc_tg <- configure_hmc_safely(
  build_fn = build_M, niter = 2200, nburnin = 200, thin = 2,
  monitors = monitor, nchains = 10,
  nuts_targets = c("logit_theta3[1]", "N2[45, 3]"),
  full_hmc = FALSE
)
```


```{r  Visualization and Diagnostics,message=FALSE, warning=FALSE,fig.cap="", out.width="90%",echo=TRUE, results='asis'}
diag_tbl <- compute_diag_from_mcmc_vect(
  samples_ml,
  runtime_s = res_b$runtime_s,
  compute_rhat = "both",
  ess_for = "both",
  target_block_ram_gb = 2
)


plots_bn <-plot_bottlenecks_fast(
  diag_tbl,
  sampled_only = TRUE,     # OK maintenant, robuste
  conf.mcmc = geremconfMCMC,
  samples_ml = samples_ml,
  out_dir = "outputs/diagnostics"
)
plots_cv <- plot_convergence_rhat_sampled_fast(
  diag_tbl,
  threshold = 1.01,
  sampled_only = TRUE,
  conf.mcmc = geremconfMCMC,
  samples_ml = samples_ml,
  out_dir = "outputs/diagnostics",
  top_k = 100L,
  prefer_split = TRUE
)

cat("\nCreated bottleneck figures (plot_bottlenecks):\n")
print(names(plots_bn))
cat("\nConvergence outputs (plot_convergence_checks):\n")
print(names(plots_cv))
cat("\nIndex panels (plot_bottlenecks_index):\n")
print(names(plots_bi))
```

