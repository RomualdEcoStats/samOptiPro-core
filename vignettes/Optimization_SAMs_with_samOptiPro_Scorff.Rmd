---
title: "Optimizing Samplers in NIMBLE/Optimization of SAMs: example of a differentiable model"
author:  "Romuald H."
date: "08/09/2020"
documentclass: article
output: 
  bookdown::pdf_document2:
    fig_caption: yes
    toc: no
    latex_engine: xelatex
   
---

```{r setup, include=FALSE}
rm(list = ls())
library(knitr)
knitr::opts_chunk$set(
  echo       = FALSE,
  warning    = FALSE,
  message    = FALSE,
  fig.width  = 14.9/2.54,
  fig.height = 10/2.54,
  dpi        = 300
)
colorpalette <- c(
  "#999999", "#E69F00", "#56B4E9", "#009E73",
  "#F0E442", "#0072B2", "#D55E00", "#CC79A7"
)
```

# Introduction

This tutorial demonstrates how to **diagnose differentiability** and **optimize MCMC sampling strategies** in complex Bayesian state–space models using the R package **`samOptiPro`** (Hounyeme *et al.*, 2025).  
We illustrate the workflow on a simple population dynamics model, using `nimble` and `nimbleHMC` as the computational back-end.

We will:

- Build and simulate a **population growth model** with a latent process and log-normal observations.  
- Identify bottlenecks (algorithmic bottlenecks and time bottlenecks).  
- Diagnose **non-differentiable components** (to decide whether HMC/NUTS is applicable).  
- Automatically **benchmark samplers** (RW, slice, AF_slice, HMC, NUTS) using `test_strategy_family()` / `test_strategy()`.  
- Assess **algorithmic efficiency (AE)** and **computational efficiency (CE)**.

All results (traceplots, diagnostics, and performance summaries) are saved under `outputs/`.

# Step 0 – Load packages

```{r load-packages}
library(coda)
library(ggplot2)
library(nimble)
library(nimbleHMC)
```

```{r load-samoptipro}
devtools::load_all("~/samOptiPro_packages_dev/samOptiPro")
devtools::document()
devtools::load_all()
```

# Step 1 – Simulated data, initial values and monitors

```{r inputs-data-monitors}
# Data simulation
datafile <- "Scorff/data_scorff_LCM_v2.rds"
Constfile <- "Scorff/Const_scorff_LCM_v2.rds"
Data_nimble <- readRDS(datafile)
Const_nimble <- readRDS(Constfile)

# Initial values
#initfile <- "Scorff/0_generate_inits_base.R"
source("Scorff/0_generate_inits_base.R")
# Monitors
monitors <- c(
  
  # Abundances
  "N1","N2","N3",
  "N3_tot","N4","N5","N6","N7","N8","N9","N10",
  "N3f_tot","N4f","N5f","N6f","N7f","N8f","N9f","N10f",
  "N3m_tot","N4m","N5m","N6m","N7m","N8m","N9m","N10m",

  # Stock recruitment
  "logN2_mean","logN2_sd",
  "alpha","k",
  
  # Smolt ages
  "p_smolt_cohort",
  "p_smolt1_migr",  "p_smolt_dirch",

  # Sex-ratio
  "prop3f","prop6f","prop9f",

  # Post-smolt survival
  "theta3","logit_theta3",
  "sd_theta3",

  # Probability of maturation as 1SW
  "theta4f","theta4m","theta4", 
  "logit_theta4f","logit_theta4m",
  "sd_theta4f","sd_theta4m","mu_logit_theta3",  "mu_logit_theta4f", "mu_logit_theta4m",

  # survival from end sum1 to return
  "theta5","theta8",
  "logit_theta5","logit_theta8",
  
  "h6_hw","h9_hw",
  "C6_hw","C9_hw"
)

```

# Step 2 – Model Scorff

```{r model-Scorff}
model.nimble <- nimbleCode({
  
  # Observation equations ---------------------------------------------------
  
  ## Total smolt abundances estimates -----
  ## we estimate the total true abundance of smolts (N3_tot) using the log of the median, the mean and the standard deviation estimated from a previous CMR model
  
  # Fit of N3_tot to available data  with observation errors (Pseudolikelihood method)
  # from 1996 to 2019
  for (t in 3:(nyear-1)) {
    log_N3_mu[t] ~ dnorm(log(N3_tot[t]), sd = log_N3_sd[t])
  }
  
  
  ## Smolt Sex ratio -----
  ## use of the number of individuals for each sex  (SAMARCH data) to estimate the sex ratio (p_sex_smolt) (multinomial)
  
  # from 1996 to 2018
  for (t in 3:(nyear-2))  {
    sex_ratio_smolt[t,1] ~ dhyperg(N1 = round(prop3f[t]*N3_tot[t]), 
                                  N0 = round((1-prop3f[t])*N3_tot[t]),
                                  nx = N_sample_smolt_sex[t])
  }
  
  ## smolt age proportion ----------------------------------------------------
  # 1997 to 2019
  for(t in 4:(nyear-1)){
    p_smolt_dirch[t,1] <- p_smolt1_migr[t]*N_sample_smolt_age
    p_smolt_dirch[t,2] <- (1-p_smolt1_migr[t])*N_sample_smolt_age
    p_smolt_migr_data[t, 1:2] ~ ddirich(p_smolt_dirch[t,1:2])
  }
  
  
  
  ## Total adult abundances estimates ----
  ## we estimate the total true abundance of adults (N6/N9) using the log of the median, the mean and the standard deviation estimated from a previous CMR model
  
  # from 1994 to 2019
  for (t in 1:(nyear-1))  {
    log_N6_mu[t] ~ dnorm(log(N6[t]), sd = log_N6_sd[t])
  }
  # from 1994 to 2020
  for (t in 1:(nyear))  {
    log_N9_mu[t] ~ dnorm(log(N9[t]), sd = log_N9_sd[t])
  }
  
  
  ## adult Sex ratio -----
  ## use of the number of individuals for each sex  (SAMARCH data) to estimate the sex ratio for 1SW and 2SW (prop6f, prop9f) (multinomial)
  
  # from 1994 to 2019
  for (t in 1:(nyear-1)){
    sex_ratio_1SW[t,1] ~ dhyperg(N1 = round(prop6f[t]*N6[t]), 
                                   N0 = round((1-prop6f[t])*N6[t]),
                                   nx = N_sample_1SW[t])
  }
  
  # from 1994 to 2020
  for (t in 1:(nyear)){
    sex_ratio_2SW[t,1] ~ dhyperg(N1 = round(prop9f[t]*N9[t]), 
                                 N0 = round((1-prop9f[t])*N9[t]),
                                 nx = N_sample_2SW[t])
    
  }
  
  
  ## Homewater Catches -------------------------------------------------------
  # from 1994 to 2019
  for (t in 1:(nyear-1))  {
    log_C6_mu[t] ~ dnorm(log(C6_hw[t]), sd = log_C6_sd[t])
  }
  # from 1994 to 2020
  for (t in 1:(nyear))  {
    log_C9_mu[t] ~ dnorm(log(C9_hw[t]), sd = log_C9_sd[t])
  }
  
  
  
  # Priors ------------------------------------------------------------------
  
  ## Stock-recruitment parameters --------------------------------------------
  
  logN2_sd ~ dunif(0,5)
  alpha ~ dbeta(theta1_max*nsample_theta1, (1-theta1_max)*nsample_theta1)
  k ~ dlnorm(meanlog = logk_pr, sdlog = 1)
  
  ## Smolt Abundance & Sex Ratio  ---------------------
  
  # year 1996 
  N3_tot[3] ~ dlnorm(0,0.01)
  
  # year 1996-2018
  for (t in 3:(nyear-2)){
    prop3f[t] ~ dbeta(10,10)
  }
  
  
  ## Smolt freshwater age proportion ----------------------------------------------------
  # 1994 to 2017
  for(t in 1:(nyear-3)){
    p_smolt_cohort[t,1] ~ dbeta(2,2)
    p_smolt_cohort[t,2] <- 1-p_smolt_cohort[t,1]
  }
  
  ## Post-smolt survival  ----------------------
  # survival during first summer (probability for individuals to survive)
  # psurv_smolt_F[t,k] is for smolt survival, in year t and size class k. 
  # Here we consider only one size class so k=1.
  
  # year 1996-2018
  for (t in 3:(nyear-2)){
    theta3[t] <- ilogit(logit_theta3[t])
    # survival in logit scale
    logit_theta3[t] ~ dnorm(mu_logit_theta3,sd = sd_theta3)
  }
  sd_theta3 ~ dunif(0,5)
  mu_logit_theta3 ~ dnorm(0,0.1)
  
  ## Probability of maturation  -----------
  # maturation during first summer (probability for individuals to mature)
  # pmat1_F[t,k] is for maturation at end of first summer, in year t and size class k. 
  
  # year 1997-2019
  for (t in 4:(nyear-1)){
    theta4f[t] <- ilogit(logit_theta4f[t])
    theta4m[t] <- ilogit(logit_theta4m[t])
    theta4[t] <- prop3f[t-1]*theta4f[t]+ (1-prop3f[t-1])*theta4m[t]
    
    #maturation in logit scale: intercept + temporal trend
    logit_theta4m[t] ~ dnorm(mu_logit_theta4m,sd = sd_theta4m)
    logit_theta4f[t] ~ dnorm(mu_logit_theta4f,sd = sd_theta4f)
    
  }
  sd_theta4f ~ dunif(0,5)
  sd_theta4m ~ dunif(0,5)
  mu_logit_theta4f ~ dnorm(0,0.1)
  mu_logit_theta4m ~ dnorm(0,0.1)
  
  ## Post PFA mortality ------------------------------------------------------
  # sync with cohort maturing year t
  # year 1997-2019
  for (t in 4:(nyear-1)){
    ### Survival probabilities -----
    # s1 and s2 are constant rates.
    # s1 is the additional survival probability during the first year at sea for 1SW adults
    # s2 is the additional survival probability during the first year at sea and during the second year at sea for 2SW adults
    # M is the mortality rate per additional month at sea after the end of first summer (november). Delta_t is the number of additional month at sea. 
    # 1SW fish return on next July, 2SW fish stay an additional year and return on the next march
    logit_theta5[t] <- logit(exp(-E_M*(delta5)))
    logit_theta8[t] <-  logit(exp(-E_M*(delta8)))
    theta5[t] <- ilogit(logit_theta5[t])
    theta8[t] <- ilogit(logit_theta8[t])
    
  }
  
  
  ## Exploitation Rates ------------------------------------------------------
  
  # year 1994-2019
  for(t in 1:(nyear-1)){
    h6_hw[t] ~ dbeta(1,2)
  }
  # year 1994-2020
  for(t in 1:(nyear)){
    h9_hw[t] ~ dbeta(1,2)
  }
  
  ## Return Abundance & sex-ratio 1994-1997 ----------------------------
  
  # 1SW : year 1994-1996
  for(t in 1:3){
    prop6f[t] ~ dunif(0,1)
    N6[t] ~ dlnorm(meanlog = 6.5, sdlog = 1)
    N6f[t] <- N6[t] * prop6f[t]
    N6m[t] <- N6[t] * ( 1 - prop6f[t] )
  }
  
  # 2SW : year 1994-1997
  for(t in 1:4){
    prop9f[t] ~ dunif(0,1)
    N9[t] ~ dlnorm(meanlog = 4.5, sdlog = 1)
    N9f[t] <- N9[t] * prop9f[t]
    N9m[t] <- N9[t] * ( 1 - prop9f[t] )
  }
  
  # Life Cycle  -----------------------------------------------------------
  # (indices are available up to n.year)
  
  ## Egg deposition and Stock-Recruitment -----------------
  
  # 1994 to 2017
  for (t in 1:(nyear-3)){
    N1[t] <- 
      N7f[t] * eggs[1,t] + 
      N10f[t] * eggs[2,t]
    
    # Stock 
    logN2_mean[t] <-  log(N1[t] / (1/alpha + N1[t]/k))
    N2[t] ~ dlnorm(logN2_mean[t]-0.5*logN2_sd^2, sd = logN2_sd)
  }
  
  ## Parr-smolt transition ---------------
  # 1994 to 2017
  for(t in 1:(nyear-3)){
    for(fw in 1:2){
      N3[t+1+fw,fw] <- N2[t]*p_smolt_cohort[t,fw]
    }
  }
  
  # 1997 to 2019
  # N3_tot[1] has a direct prior
  for(t in 4:(nyear-1)){
    N3_tot[t] <- N3[t,1] + N3[t,2]
    p_smolt1_migr[t] <- N3[t,1]/N3_tot[t]
  }
  for(t in 3:(nyear-2)){
    N3f_tot[t] <- N3_tot[t] * prop3f[t]
    N3m_tot[t] <- N3_tot[t] * (1 - prop3f[t])
  }
  
  ## post-smolt survival -----------------
  ### N4 is PFA
  # N3_tot: 1996 to 2018
  for (t in 3:(nyear-2)){
    N4f[t+1] <- N3f_tot[t] * theta3[t]
    N4m[t+1] <- N3m_tot[t] * theta3[t]
    N4[t+1]  <- N4m[t+1]   + N4f[t+1]
  }
  
  ## maturation -----------------
  # 1997 to 2019
  for (t in 4:(nyear-1)){
    # Dynamic equation for maturing individuals
    # Total number of maturing fish
    N5f[t] <- N4f[t]  * theta4f[t]  
    N5m[t] <- N4m[t]  * theta4m[t] 
    N5[t] <- N5f[t]+N5m[t]
    # Total number of non maturing fish
    N8f[t] <-  N4f[t]  * (1-theta4f[t])
    N8m[t] <-  N4m[t]  * (1-theta4m[t]) 
    N8[t] <- N8f[t]+N8m[t]
  }
  
  
  ### total numbers N6f, N6m, N9f, N9m
  #total number of individuals that survived grew and matured in each sea age and sex categories should be multiplied by the additional survival, differently for 1SW and 2SW
  
  # 1997 to 2019
  for (t in 4:(nyear-1))
  {	
    # 1SW F and M (maturing individuals * survival s1 between sum1 and return)
    N6f[t] <-  N5f[t] * theta5[t]
    N6m[t] <-  N5m[t] * theta5[t]
    N6[t] <- N6f[t]+N6m[t]
    prop6f[t] <- N6f[t]/N6[t] 
    
    # 2SW F and M (non maturing individuals * survival s2 between sum1 and return)
    N9f[t+1] <-  N8f[t] * theta8[t]
    N9m[t+1] <-  N8m[t] * theta8[t]
    N9[t+1] <- N9f[t+1]+N9m[t+1]
    prop9f[t+1] <- N9f[t+1]/N9[t+1]
  }
  
  
  ## Homewater Catches  ------------------------------------------------------
  # 1994 to 2019
  for(t in 1:(nyear-1)){
    C6_hw[t] <- N6[t]*h6_hw[t]
    N7f[t] <- N6f[t]*(1-h6_hw[t])
    N7m[t] <- N6m[t]*(1-h6_hw[t])
    N7[t] <- N7f[t]+N7m[t]
  }
  # 1994 to 2020
  for(t in 1:(nyear)){
    C9_hw[t] <- N9[t]*h9_hw[t]
    N10f[t] <- N9f[t]*(1-h9_hw[t])
    N10m[t] <- N9m[t]*(1-h9_hw[t])
    N10[t] <- N10f[t]+N10m[t]
  }
  
  
  ## End of population dynamic process  ----------------
  # end model
})

```

# Step 3 – Building and compiling the model

The function `build_M()` is the central entry point used by most `samOptiPro`
tools. It returns the model, its compiled version, the default monitors and the
model code as plain text.

```{r build-compile}
nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)
nimbleOptions(MCMCsaveHistory = FALSE)


# nlchoose - Binomial coefficient calculator ---------------
nlchoose <- nimbleFunction(
  run = function(n = double(0), k = double(0)){
    returnType(double(0))
    k <- round(k)
    n <- round(n)
    if(n==k | k==0) return(0)
    else if(n < k) return(-Inf)
    else return(sum(log((n-k+1):n)) - sum(log(1:k)))
  }
) 

# dhyperg - hypergeometric density function ---------------
dhyperg <- nimbleFunction(
  run = function(x = double(0), nx = double(0), N1 = double(0), N0 = double(0),
                 log = integer(0, default = 1)){
    returnType(double(0))
    logProb <- nlchoose(N1,x) + nlchoose(N0,nx-x) - nlchoose(N0+N1,nx)
    if(log) return(logProb)
    else return(exp(logProb))
  })

# rhyperg - hypergeometric RNG -------------
rhyperg <- nimbleFunction(
  run = function(n = integer(0, default = 1), nx = double(0), N1 = double(0), N0 = double(0)){
    returnType(double(0))
    if(n!=1) print('rhyperg only allows n=1; using n = 1')
    dev <- runif(1,0,1)
    minx <- max(nx - N0, 0)
    maxx <- min(nx, N1)
    xs <- numeric(length = maxx-minx+1)
    ps <- xs
    xs[1] <- minx
    ps[1] <- dhyperg(minx, nx, N1, N0, log = 0)
    
    for(i in (minx+1):maxx){
      xs[i-minx+1] <- i
      ps[i-minx+1] <- dhyperg(i, nx, N1, N0, log = 0) + ps[i-minx]
    }
    
    # return(max(xs[dev > ps]))
    return(xs[which(! dev > ps)[1]])
  }
)

# register Nimble functions to R's global environment ----------------
assign('nlchoose', nlchoose, .GlobalEnv)
assign('dhyperg', dhyperg, .GlobalEnv)
assign('rhyperg', rhyperg, .GlobalEnv)


m <- nimbleModel(
  code        = model.nimble,
  name        = "model.nimble",
  constants   = Const_nimble,
  data        = Data_nimble,
  inits       = myinits,
  buildDerivs = TRUE
)

cm <- compileNimble(m)

build_M <- function() list(
  model     = m,
  cmodel    = cm,
  monitors  = monitors,
  code_text = paste(deparse(model.nimble), collapse = "\n")
)
```

# Step 4 – Diagnosing differentiability and HMC eligibility

## `diagnose_model_structure()`: structural and timing bottlenecks

The function `diagnose_model_structure()` inspects the **graph structure** of
a NIMBLE model and returns a set of tables describing:

- the list of stochastic and deterministic nodes;  
- the dependency graph (downstream nodes for each variable);  
- a decomposition of node families (e.g. `logit_theta`, `N`, `theta`) and
  their dependency counts;  
- optional timing information for samplers (via an internal profiling run).  

Conceptually, `diagnose_model_structure()` is **purely structural**: it only
needs a compiled model, not MCMC samples. It is therefore cheap and can be used
early, even before running any long MCMC.

In practice, we call it as follows for the M3 model:

```{r diagnose-structure, message=FALSE, warning=FALSE}
cat("\n[MODEL STRUCTURE CHECK]\n")
diag_s <- diagnose_model_structure(
  model              = m,
  include_data       = FALSE,
  removed_nodes      = NULL,
  ignore_patterns    = c("^lifted_", "^logProb_"),
  make_plots         = TRUE,
  output_dir         = "outputs/diagnostics",
  save_csv           = FALSE,
  node_of_interest   = NULL,
  sampler_times      = NULL,
  sampler_times_unit = "seconds",
  auto_profile       = TRUE,
  profile_niter      = 5000,
  profile_burnin     = 500,
  profile_thin       = 1,
  profile_seed       = NULL,
  np                 = 0.10,
  by_family          = TRUE,
  family_stat        = c("median", "mean", "sum"),
  time_normalize     = c("none", "per_node"),
  only_family_plots  = TRUE
)
```

## `run_structure_and_hmc_test()`: combined structure + HMC/NUTS check

In practice, `run_structure_and_hmc_test()` is a convenience wrapper:
it calls `diagnose_model_structure()` and, when possible, attempts a **full‑model
HMC/NUTS configuration**. This gives, in a single object `out`, both the
structural bottlenecks and an early indication of whether a global HMC strategy
is realistic for the current model.

```{r structure-hmc-test, message=FALSE, warning=FALSE}
out <- run_structure_and_hmc_test(build_M, include_data = FALSE)
```

# Step 5 – Baseline MCMC, bottlenecks and performance assessment

## `run_baseline_config()`: a safe default MCMC runner

`run_baseline_config()` is the main entry point to run a **baseline NIMBLE
configuration** on a given model:

- it calls your model builder (here `build_M()`);  
- it configures a default sampler strategy (typically scalar RW, possibly with
  default slice samplers);  
- it compiles the MCMC and runs it for the requested number of iterations and chains;  
- it returns the raw samples together with the total runtime.

In this vignette, we use it to generate a long baseline run for model M3.
Because this can be time‑consuming, we mark the chunk as `eval = FALSE` so that
the vignette compiles quickly while still showing the full code.

```{r baseline-run-M3, eval=FALSE}
## MCMC schedule for the baseline run
n.iter   <- 500e3
n.burnin <- n.iter*0.3
n.thin   <- 2
n.chains <- 3

## Baseline configuration and run
res_a <- run_baseline_config(
  build_fn = build_M,
  niter    = n.iter,
  nburnin  = n.burnin,
  thin     = n.thin,
  nchains  = n.chains,
  monitors = monitors
)

## Merge samples / samples2 into a clean mcmc.list
samples_mla <- as_mcmc_list_sop(
  res_a$samples,
  res_a$samples2,
  drop_loglik = FALSE,
  thin        = n.thin
)

## Total runtime (seconds) for this baseline configuration
runtime_s_a <- res_a$runtime_s

## High-level performance summary (ESS / ESS/s / Rhat)
ap <- assess_performance(
  samples   = samples_mla,
  runtime_s = runtime_s_a
)

## Node-level bottlenecks (ESS & Rhat), working at the target level
bot <- identify_bottlenecks(
  samples           = samples_mla,
  runtime_s         = runtime_s_a,
  ess_threshold     = 1000,
  sampler_params    = NULL,
  model             = m,
  mcmc_conf         = NULL,
  ignore_patterns   = c("^lifted_", "^logProb_"),
  strict_sampler_only = TRUE,
  auto_configure      = TRUE,
  rhat_threshold      = 1.01,
  ess_per_s_min       = 0
)

runtime_s_a      # total runtime
ap$summary       # global performance summary
bot$top3         # worst nodes (ESS/Rhat) at the target level

## Family-level bottlenecks (aggregated by parameter family)
bot2 <- identify_bottlenecks_family(
  samples           = samples_mla,
  runtime_s         = runtime_s_a,
  ess_threshold     = 1000,
  sampler_params    = NULL,
  model             = m,
  mcmc_conf         = NULL,
  ignore_patterns   = c("^lifted_", "^logProb_"),
  strict_sampler_only = TRUE,
  auto_configure      = TRUE,
  rhat_threshold      = 1.01,
  ess_per_s_min       = 0
)

bot2$top3        # worst families by AE/CE and Rhat
```

**Summary.** The object `res_a` returned by `run_baseline_config()` is then
the **baseline reference** for the whole pipeline:

- `samples`, `samples2` are the raw draws used to derive ESS and R-hat;  
- `runtime_s` is the total runtime used to compute computational efficiency;  
- downstream tools such as `assess_performance()`, `identify_bottlenecks()`
  and `identify_bottlenecks_family()` reuse these outputs to locate structural
  and algorithmic bottlenecks in model M3.

# Step 6 – Adaptive block strategy: `test_strategy_family()` and `test_strategy()`

## `test_strategy_family()`: surgical strategy search by family

The core function of the optimisation pipeline is `test_strategy_family()`.
Given a **family of parameters** (e.g. `logit_theta`) and a baseline run, it:

1. inspects the current samplers used for this family;  
2. proposes alternative samplers (e.g. scalar slice or block-wise NUTS);  
3. runs short **pilot chains** under each strategy;  
4. compares per-parameter ESS, AE and CE against the baseline;  
5. returns a structured object that can be further plotted or summarised.

In the M3 toy example, one can use the legacy interface as:

```{r strategy-family, eval=FALSE}
diff <- test_strategy_family(
  build_fn            = build_M,
  monitors            = NULL,   # optional, just passed through
  try_hmc             = FALSE,   # only used for full-model path; surgical ignores
  nchains             = 3,
  pilot_niter         = 500000,
  pilot_burnin        = 150000,
  thin                = 1,
  out_dir             = "outputs/diagnostics",
  nbot                = 2,
  strict_scalar_seq   = "slice", #c("NUTS","slice","RW"),
  strict_block_seq    = NULL, #c("NUTS_block","AF_slice","RW_block"),
  force_families      = c("alpha","k"),   # e.g. c("logit_theta","N")
  force_nodes         = NULL,   # e.g. list(logit_theta=c("logit_theta[1]",...))
  force_union         = NULL,   # e.g. c("logit_theta","N")
  ask                 = TRUE,
  ask_before_hmc      = FALSE,
  block_max           = 20,
  slice_control       = list(),
  rw_control          = list(),
  rwblock_control     = list(adaptScaleOnly = TRUE),
  af_slice_control    = list(),
  slice_max_contractions = 5000
)
```

A more modern call uses `test_strategy_family()` directly, possibly **forcing**
specific families such as `logit_theta` and `N` to NUTS/NUTS_block:

For **singletons** or more local interventions, `test_strategy()` offers a
similar interface but at the node level:

```{r strategy-singletons, eval=FALSE}
diff2 <- test_strategy(
  build_fn          = build_M,
  monitors          = NULL,
  try_hmc           = FALSE,
  nchains           = 3,
  pilot_niter       = 4000,
  pilot_burnin      = 1000,
  thin              = 2,
  out_dir           = "outputs/diagnostics",
  nbot              = 1,
  strict_scalar_seq = c("NUTS","slice","RW"),
  strict_block_seq  = c("NUTS_block","AF_slice","RW_block"),
  force_union       = NULL,
  ask               = TRUE,
  ask_before_hmc    = TRUE,
  block_max         = 20,
  slice_control     = list(),
  rw_control        = list(),
  rwblock_control   = list(adaptScaleOnly = TRUE),
  af_slice_control  = list(),
  slice_max_contractions = 5000
)
```

The returned objects (e.g. `diff`, `diff2`, `diff3`) typically contain:

- `baseline$diag_tbl`: diagnostics for the baseline strategy (restricted to
  the family or nodes of interest);  
- `steps`: a list of strategy steps, each with its own diagnostics and
  bookkeeping information (sampler types, proposal scales, etc.).  

In short, `test_strategy_family()` and `test_strategy()` automate the
**surgical replacement of samplers** on a selected subset of nodes while
keeping the rest of the model unchanged.

Here:

- `res_fast` bundles baseline and alternative strategies;  
- `per` controls whether AE/CE/R-hat are aggregated per parameter (`"target"`)
  or per family;  
- `top_k` and `top_by` restrict the plots to the most interesting keys
  according to CE (default) or AE.  

Again, we use `eval = FALSE` to avoid long pilot runs during vignette
compilation, while preserving executable code for practical use.

# Step 7 – Visualization and diagnostics

## `compute_diag_from_mcmc()` and `compute_diag_from_mcmc_vect()`

At several steps in the workflow, we need to recompute convergence metrics
from `coda::mcmc.list` objects. `compute_diag_from_mcmc_vect()` provides a
vectorised and reasonably fast implementation for:

- minimum ESS across chains per parameter;  
- efficiency per iteration `AE_ESS_per_it`;  
- efficiency per second `ESS_per_sec`;  
- Gelman–Rubin diagnostic \(\hat{R}\) per parameter.

Below we illustrate how these diagnostics are combined with plotting utilities
from `samOptiPro`. Because this chunk depends on the baseline run, we also mark
it as `eval = FALSE`.

```{r viz-diagnostics, echo=TRUE, eval=FALSE}

## Recompute diagnostics from the baseline samples (illustration)
## Or, using the merged mcmc.list

diag_tbl <- compute_diag_from_mcmc_alt(
  samples   = samples_mla,
  runtime_s = res_a$runtime_s
)
conf.mcmc <- nimble::configureMCMC(m)

plots_bn <- plot_bottlenecks(
  diag_tbl,
  sampled_only = TRUE,            # set TRUE for sampled-only nodes
  conf.mcmc    = conf.mcmc,
  samples_ml   = samples_mla
)

plots_cv <- plot_convergence_checks(
  samples_mla,
  out_dir             = "outputs/diagnostics",
  top_k_rhat          = 8,
  top_k_aelow         = 8,
  runtime_s           = runtime_s_a,
  rhat_ref            = 1.01,
  make_rhat_hist      = TRUE,
  make_traces_rhat    = TRUE,
  make_traces_ae      = TRUE,
  make_rhat_family_bars = TRUE
)

plots_bi <- plot_bottlenecks_index(
  diag_tbl,
  out_dir      = "outputs/diagnostics",
  top_k        = 20L,
  make_hist_ce = TRUE,
  make_hist_ae = TRUE
)

cat("\nCreated bottleneck figures (plot_bottlenecks):\n")
print(names(plots_bn))
cat("\nConvergence outputs (plot_convergence_checks):\n")
print(names(plots_cv))
cat("\nIndex panels (plot_bottlenecks_index):\n")
print(names(plots_bi))
```

This is exactly the kind of diagnostic object expected by the strategy-testing
functions described above.

# 8. Conclusions

This workflow highlights how `samOptiPro` helps to:

- Detect non-differentiable nodes that prevent HMC/NUTS usage.  
- Automatically switch between gradient-based and non-gradient samplers.  
- Quantify algorithmic and computational efficiency for each parameter family.  
- Provide transparent diagnostic plots and benchmark reports.  
- Use adaptive block design to ensure faster convergence and improved mixing
  without manual tuning.

Putting everything together:

1. `run_baseline_config()` gives a robust starting point and a first diagnostic
   table (`diag_tbl`).  
2. `diagnose_model_structure()` reveals structural and timing bottlenecks at
   the graph level.  
3. `compute_diag_from_mcmc_vect()` allows you to recompute diagnostics from any
   set of samples and a runtime.  
4. `test_strategy_family()` lets you surgically optimise samplers for a given
   family such as `logit_theta`.  
5. `test_strategy_family_fast()` and
   `plot_strategies_from_test_result_fast()` provide a fast, visual way to
   compare multiple sampler strategies side‑by‑side.  

The M3 toy model illustrates this workflow on a small, controlled example, but
the same sequence of steps scales to much larger hierarchical models, including
real‑world stock assessment and population dynamics models.

# 9. Other useful `samOptiPro` tools

In this last section, we briefly summarise additional `samOptiPro` functions
that are not fully showcased in the M3 workflow but are useful building blocks
for more advanced applications:

- **`checkInits()` / `checkInitsAndRun()`**  
  Robust helpers to validate initial values against a NIMBLE model, with clear
  error messages and optional automatic launching of the MCMC once inits are
  deemed valid.

- **`run_hmc_all_nodes()`**  
  Driver to attempt full‑model HMC on differentiable DAGs, using global
  Hamiltonian proposals while keeping track of failures and fallbacks.

- **`run_baseline_coda()`**  
  Alternative baseline runner that focuses on standard `coda` workflows, useful
  when existing MCMC runs already exist outside `samOptiPro` but one wishes to
  reuse its diagnostic and plotting tools.

- **`as_mcmc_list_sop()`**  
  Flexible converter that merges primary and secondary monitor sets into a
  single `coda::mcmc.list`, ensuring consistent parameter ordering across
  chains and downstream diagnostics.

- **`plot_strategies_from_test_result_hmc_fast()`**  
  A plotting companion focusing on HMC/NUTS‑specific trials, providing quick
  visual comparisons of AE/CE and convergence metrics for gradient‑based
  strategies.

- **`Other functions can be found here`**: ~/samOptiPro_packages_dev/samOptiPro/docs/reference

These tools, together with the core workflow illustrated on model M3, form a
coherent ecosystem for **designing, diagnosing and optimising** complex
Bayesian samplers in `nimble`‑based ecological and stock‑assessment models.

# 10. References
